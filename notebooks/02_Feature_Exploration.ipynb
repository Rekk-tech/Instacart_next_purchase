{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0999c4ff",
   "metadata": {},
   "source": [
    "## 1. Load Real Instacart Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6cfe338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Feature importance libraries\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97a75aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Setting up Google Colab environment...\n",
      "âš ï¸ Drive mount issue: mount failed\n",
      "ğŸ’¡ You may need to authorize access manually\n",
      "âœ… Required packages already available!\n",
      "âš ï¸ Drive path not found, trying local paths...\n",
      "âœ… Processed directory created: ../data/processed\n",
      "\n",
      "ğŸ“‚ Checking data availability...\n",
      "Raw data path: ../data/raw\n",
      "Processed data path: ../data/processed\n",
      "âŒ Raw data directory not found!\n",
      "ğŸ”§ Please check data location or upload to correct path\n",
      "ğŸ“‹ Required files: orders.csv, order_products__prior.csv, order_products__train.csv, products.csv, departments.csv, aisles.csv\n",
      "\n",
      "ğŸ¯ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# AUTO SETUP FOR GOOGLE COLAB\n",
    "print(\"ğŸš€ Auto-detecting environment and setting up...\")\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Check if running in Google Colab\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    IN_COLAB = True\n",
    "    print(\"âœ… Running in Google Colab\")\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "    print(\"âš ï¸ Not in Colab - using local environment\")\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Auto mount Google Drive\n",
    "    if not os.path.exists('/content/drive'):\n",
    "        print(\"ğŸ”„ Mounting Google Drive...\")\n",
    "        try:\n",
    "            drive.mount('/content/drive')\n",
    "            print(\"âœ… Drive mounted successfully!\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Mount failed: {e}\")\n",
    "            print(\"ğŸ’¡ Please run: drive.mount('/content/drive') manually\")\n",
    "    else:\n",
    "        print(\"âœ… Drive already mounted\")\n",
    "\n",
    "# Install missing packages for Colab\n",
    "if IN_COLAB:\n",
    "    print(\"ğŸ“¦ Installing packages for Colab...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    try:\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"plotly\", \"xgboost\"])\n",
    "        print(\"âœ… Packages installed successfully!\")\n",
    "    except:\n",
    "        print(\"âš ï¸ Some packages may already be installed\")\n",
    "else:\n",
    "    print(\"âš ï¸ Skipping package installation for local environment\")\n",
    "\n",
    "# Smart path detection\n",
    "print(\"\\nğŸ“‚ Detecting data paths...\")\n",
    "possible_paths = [\n",
    "    (\"/content/drive/MyDrive/instacart_data/raw\", \"/content/drive/MyDrive/instacart_data/processed\"),  # Colab Drive\n",
    "    (\"/content/drive/My Drive/instacart_data/raw\", \"/content/drive/My Drive/instacart_data/processed\"),  # Space in name\n",
    "    (\"/content/data/raw\", \"/content/data/processed\"),  # Direct upload to Colab\n",
    "    (\"../data/raw\", \"../data/processed\"),  # Local\n",
    "]\n",
    "\n",
    "data_raw = None\n",
    "data_processed = None\n",
    "\n",
    "for raw_path, proc_path in possible_paths:\n",
    "    if Path(raw_path).exists():\n",
    "        data_raw = Path(raw_path)\n",
    "        data_processed = Path(proc_path)\n",
    "        print(f\"âœ… Found data at: {data_raw}\")\n",
    "        break\n",
    "    else:\n",
    "        print(f\"âŒ Not found: {raw_path}\")\n",
    "\n",
    "if data_raw is None:\n",
    "    # Default to Colab path if in Colab, local otherwise\n",
    "    if IN_COLAB:\n",
    "        data_raw = Path(\"/content/drive/MyDrive/instacart_data/raw\")\n",
    "        data_processed = Path(\"/content/drive/MyDrive/instacart_data/processed\")\n",
    "    else:\n",
    "        data_raw = Path(\"../data/raw\")\n",
    "        data_processed = Path(\"../data/processed\")\n",
    "    print(f\"âš ï¸ Using default path: {data_raw}\")\n",
    "\n",
    "# Create processed directory\n",
    "try:\n",
    "    data_processed.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"âœ… Processed directory ready: {data_processed}\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not create processed directory: {e}\")\n",
    "\n",
    "# Check data availability\n",
    "print(f\"\\nğŸ“‚ Checking data availability...\")\n",
    "required_files = [\"orders.csv\", \"order_products__prior.csv\", \"order_products__train.csv\", \n",
    "                 \"products.csv\", \"departments.csv\", \"aisles.csv\"]\n",
    "\n",
    "if data_raw.exists():\n",
    "    print(\"âœ… Raw data directory found!\")\n",
    "    found_files = list(data_raw.glob(\"*.csv\"))\n",
    "    print(f\"ğŸ“ Found {len(found_files)} CSV files:\")\n",
    "    \n",
    "    missing_files = []\n",
    "    for req_file in required_files:\n",
    "        file_path = data_raw / req_file\n",
    "        if file_path.exists():\n",
    "            file_size = file_path.stat().st_size / (1024*1024)  # MB\n",
    "            print(f\"   âœ… {req_file} ({file_size:.1f} MB)\")\n",
    "        else:\n",
    "            missing_files.append(req_file)\n",
    "            print(f\"   âŒ {req_file} - MISSING\")\n",
    "    \n",
    "    if missing_files:\n",
    "        print(f\"\\nâš ï¸ Missing {len(missing_files)} required files:\")\n",
    "        for f in missing_files:\n",
    "            print(f\"   - {f}\")\n",
    "        print(\"\\nğŸ”§ Solutions:\")\n",
    "        if IN_COLAB:\n",
    "            print(\"   1. Ensure shared folder is copied to 'My Drive'\")\n",
    "            print(\"   2. Check folder structure: MyDrive/instacart_data/raw/\")\n",
    "            print(\"   3. Verify all files were uploaded completely\")\n",
    "        else:\n",
    "            print(\"   1. Check ../data/raw/ directory exists\")\n",
    "            print(\"   2. Verify file permissions\")\n",
    "    else:\n",
    "        print(f\"\\nğŸ‰ All required files found and ready!\")\n",
    "else:\n",
    "    print(\"âŒ Raw data directory not found!\")\n",
    "    if IN_COLAB:\n",
    "        print(\"ğŸ”§ For Google Colab:\")\n",
    "        print(\"   1. Copy the shared folder to your 'My Drive'\")\n",
    "        print(\"   2. Rename folder to exactly 'instacart_data'\")\n",
    "        print(\"   3. Ensure structure: MyDrive/instacart_data/raw/*.csv\")\n",
    "        print(\"   4. Try manual mount cell below if needed\")\n",
    "    else:\n",
    "        print(\"ğŸ”§ For local environment:\")\n",
    "        print(\"   1. Ensure data is in ../data/raw/ directory\")\n",
    "        print(\"   2. Check file permissions\")\n",
    "\n",
    "print(f\"\\nğŸ¯ Setup complete!\")\n",
    "print(f\"ğŸ“ Current paths:\")\n",
    "print(f\"   Raw: {data_raw}\")\n",
    "print(f\"   Processed: {data_processed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb66c2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING REAL INSTACART DATA FOR FEATURE EXPLORATION\n",
      "============================================================\n",
      "Loading real Instacart datasets...\n",
      "Loading orders.csv...\n",
      "âŒ Error loading data: [Errno 2] No such file or directory: '/content/drive/MyDrive/instacart_data/raw/orders.csv'\n",
      "Please check that all data files exist in the raw data directory\n"
     ]
    }
   ],
   "source": [
    "# Load REAL Instacart data for feature engineering\n",
    "print(\"LOADING REAL INSTACART DATA FOR FEATURE EXPLORATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Use paths from setup cell (already defined globally)\n",
    "print(f\"ğŸ“‚ Using data paths:\")\n",
    "print(f\"   Raw: {data_raw}\")\n",
    "print(f\"   Processed: {data_processed}\")\n",
    "\n",
    "# Ensure processed directory exists\n",
    "data_processed.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"\\nLoading real Instacart datasets...\")\n",
    "\n",
    "# Load raw data files (the complete real dataset)\n",
    "try:\n",
    "    # Core datasets with progress indicators\n",
    "    print(\"ğŸ“Š Loading orders.csv...\")\n",
    "    orders = pd.read_csv(data_raw / \"orders.csv\")\n",
    "    print(f\"   âœ“ Loaded {orders.shape[0]:,} orders\")\n",
    "    \n",
    "    print(\"ğŸ“Š Loading order_products__prior.csv...\")\n",
    "    order_products_prior = pd.read_csv(data_raw / \"order_products__prior.csv\")\n",
    "    print(f\"   âœ“ Loaded {order_products_prior.shape[0]:,} prior order-products\")\n",
    "    \n",
    "    print(\"ğŸ“Š Loading order_products__train.csv...\")\n",
    "    order_products_train = pd.read_csv(data_raw / \"order_products__train.csv\")\n",
    "    print(f\"   âœ“ Loaded {order_products_train.shape[0]:,} train order-products\")\n",
    "    \n",
    "    print(\"ğŸ“Š Loading products.csv...\")\n",
    "    products = pd.read_csv(data_raw / \"products.csv\")\n",
    "    print(f\"   âœ“ Loaded {products.shape[0]:,} products\")\n",
    "    \n",
    "    print(\"ğŸ“Š Loading departments.csv...\")\n",
    "    departments = pd.read_csv(data_raw / \"departments.csv\")\n",
    "    print(f\"   âœ“ Loaded {departments.shape[0]:,} departments\")\n",
    "    \n",
    "    print(\"ğŸ“Š Loading aisles.csv...\")\n",
    "    aisles = pd.read_csv(data_raw / \"aisles.csv\")\n",
    "    print(f\"   âœ“ Loaded {aisles.shape[0]:,} aisles\")\n",
    "    \n",
    "    print(\"\\nâœ… Successfully loaded all raw data files!\")\n",
    "    \n",
    "    # Display dataset sizes and insights\n",
    "    print(f\"\\nğŸ“ˆ REAL DATASET SUMMARY:\")\n",
    "    print(f\"  ğŸ“¦ Orders: {orders.shape[0]:,}\")\n",
    "    print(f\"  ğŸ‘¥ Users: {orders['user_id'].nunique():,}\")\n",
    "    print(f\"  ğŸ›’ Products: {products.shape[0]:,}\")\n",
    "    print(f\"  ğŸ“‹ Prior order-product records: {order_products_prior.shape[0]:,}\")\n",
    "    print(f\"  ğŸ¯ Train order-product records: {order_products_train.shape[0]:,}\")\n",
    "    print(f\"  ğŸª Departments: {departments.shape[0]:,}\")\n",
    "    print(f\"  ğŸ›ï¸ Aisles: {aisles.shape[0]:,}\")\n",
    "    \n",
    "    # Real data patterns analysis\n",
    "    print(f\"\\nğŸ“Š DATA PATTERNS ANALYSIS:\")\n",
    "    \n",
    "    # Calculate basket sizes from prior orders\n",
    "    prior_basket_sizes = order_products_prior.groupby('order_id').size()\n",
    "    avg_basket_size = prior_basket_sizes.mean()\n",
    "    \n",
    "    # Calculate reorder rate\n",
    "    reorder_rate = order_products_prior['reordered'].mean()\n",
    "    \n",
    "    # Calculate orders per user (only prior orders)\n",
    "    prior_orders = orders[orders['eval_set'] == 'prior']\n",
    "    orders_per_user = prior_orders.groupby('user_id').size()\n",
    "    avg_orders_per_user = orders_per_user.mean()\n",
    "    \n",
    "    print(f\"  ğŸ›’ Average basket size: {avg_basket_size:.2f} items\")\n",
    "    print(f\"  ğŸ”„ Reorder rate: {reorder_rate:.1%}\")\n",
    "    print(f\"  ğŸ“Š Average orders per user: {avg_orders_per_user:.2f}\")\n",
    "    print(f\"  ğŸ“ˆ Max orders per user: {orders_per_user.max()}\")\n",
    "    print(f\"  ğŸ“‰ Min orders per user: {orders_per_user.min()}\")\n",
    "    \n",
    "    # Time patterns\n",
    "    print(f\"\\nâ° TIME PATTERNS:\")\n",
    "    peak_hours = orders['order_hour_of_day'].mode().values\n",
    "    popular_days = orders['order_dow'].mode().values\n",
    "    print(f\"  ğŸ•™ Peak shopping hours: {peak_hours}\")\n",
    "    print(f\"  ğŸ“… Most popular shopping days: {popular_days}\")\n",
    "    \n",
    "    # Product insights\n",
    "    print(f\"\\nğŸ† TOP PRODUCTS:\")\n",
    "    top_products = order_products_prior['product_id'].value_counts().head(5)\n",
    "    for product_id, count in top_products.items():\n",
    "        product_name = products[products['product_id'] == product_id]['product_name'].iloc[0]\n",
    "        print(f\"    {product_name}: {count:,} orders\")\n",
    "    \n",
    "    # Memory usage info\n",
    "    total_memory = (orders.memory_usage(deep=True).sum() + \n",
    "                   order_products_prior.memory_usage(deep=True).sum() + \n",
    "                   products.memory_usage(deep=True).sum()) / 1024**2\n",
    "    print(f\"\\nğŸ’¾ Memory usage: {total_memory:.1f} MB\")\n",
    "    \n",
    "    print(f\"\\nğŸ‰ Real Instacart data loaded successfully!\")\n",
    "    print(\"ğŸš€ Ready for feature engineering with authentic data patterns!\")\n",
    "    \n",
    "except FileNotFoundError as e:\n",
    "    print(f\"âŒ ERROR: {e}\")\n",
    "    print(f\"\\nğŸ”§ TROUBLESHOOTING:\")\n",
    "    if IN_COLAB:\n",
    "        print(\"For Google Colab:\")\n",
    "        print(\"1. Ensure the shared folder is added to your Drive\")\n",
    "        print(\"2. Check the folder name is exactly 'instacart_data'\")\n",
    "        print(\"3. Verify the structure: MyDrive/instacart_data/raw/*.csv\")\n",
    "        print(\"4. Try running the manual mount cell\")\n",
    "        print(\"5. Check file sharing permissions\")\n",
    "    else:\n",
    "        print(\"For local environment:\")\n",
    "        print(\"1. Verify ../data/raw/ directory exists\")\n",
    "        print(\"2. Check all 6 CSV files are present\")\n",
    "        print(\"3. Verify file permissions\")\n",
    "    \n",
    "    print(f\"\\nğŸ“‹ Required files in {data_raw}:\")\n",
    "    required = [\"orders.csv\", \"order_products__prior.csv\", \"order_products__train.csv\", \n",
    "               \"products.csv\", \"departments.csv\", \"aisles.csv\"]\n",
    "    for f in required:\n",
    "        print(f\"   - {f}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Unexpected error: {e}\")\n",
    "    print(\"ğŸ’¡ Try restarting the runtime and running cells again\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5f9470",
   "metadata": {},
   "source": [
    "## 2. XÃ¢y dá»±ng User-Level Features\n",
    "\n",
    "**User-level features** mÃ´ táº£ hÃ nh vi mua sáº¯m tá»•ng thá»ƒ cá»§a tá»«ng user:\n",
    "- Sá»‘ Ä‘Æ¡n Ä‘Ã£ mua\n",
    "- Táº§n suáº¥t mua hÃ ng \n",
    "- Khoáº£ng cÃ¡ch giá»¯a cÃ¡c Ä‘Æ¡n hÃ ng\n",
    "- Sá»‘ sáº£n pháº©m Ä‘Ã£ mua\n",
    "- Tá»‰ lá»‡ reorder chung cá»§a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3af6b9e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING USER-LEVEL FEATURES\n",
      "========================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'orders' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-3406610011.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Get prior orders for feature engineering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprior_orders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eval_set'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'prior'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# 1. Sá»‘ Ä‘Æ¡n Ä‘Ã£ mua (Number of orders per user)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'orders' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"BUILDING USER-LEVEL FEATURES\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# Get prior orders for feature engineering\n",
    "prior_orders = orders[orders['eval_set'] == 'prior'].copy()\n",
    "\n",
    "# 1. Sá»‘ Ä‘Æ¡n Ä‘Ã£ mua (Number of orders per user)\n",
    "user_orders_count = prior_orders.groupby('user_id').size().reset_index(name='user_total_orders')\n",
    "\n",
    "# 2. Táº§n suáº¥t mua hÃ ng (Average days between orders)\n",
    "user_avg_days_between_orders = prior_orders.groupby('user_id')['days_since_prior_order'].mean().reset_index()\n",
    "user_avg_days_between_orders.columns = ['user_id', 'user_avg_days_between_orders']\n",
    "user_avg_days_between_orders['user_avg_days_between_orders'] = user_avg_days_between_orders['user_avg_days_between_orders'].fillna(0)\n",
    "\n",
    "# 3. Khoáº£ng cÃ¡ch tá»‘i thiá»ƒu vÃ  tá»‘i Ä‘a giá»¯a cÃ¡c Ä‘Æ¡n hÃ ng\n",
    "user_days_stats = prior_orders.groupby('user_id')['days_since_prior_order'].agg(['min', 'max', 'std']).reset_index()\n",
    "user_days_stats.columns = ['user_id', 'user_min_days_between_orders', 'user_max_days_between_orders', 'user_std_days_between_orders']\n",
    "user_days_stats = user_days_stats.fillna(0)\n",
    "\n",
    "# 4. Thá»i gian mua sáº¯m Æ°a thÃ­ch\n",
    "user_hour_mode = prior_orders.groupby('user_id')['order_hour_of_day'].apply(lambda x: x.mode().iloc[0] if not x.empty else 10).reset_index()\n",
    "user_hour_mode.columns = ['user_id', 'user_preferred_hour']\n",
    "\n",
    "user_dow_mode = prior_orders.groupby('user_id')['order_dow'].apply(lambda x: x.mode().iloc[0] if not x.empty else 0).reset_index()\n",
    "user_dow_mode.columns = ['user_id', 'user_preferred_dow']\n",
    "\n",
    "# 5. Sá»‘ sáº£n pháº©m Ä‘Ã£ mua (tá»« order_products_prior)\n",
    "# Total products per user\n",
    "user_products_count = order_products_prior.groupby('order_id')['product_id'].count().reset_index()\n",
    "user_products_count = user_products_count.merge(prior_orders[['order_id', 'user_id']], on='order_id')\n",
    "user_total_products = user_products_count.groupby('user_id')['product_id'].sum().reset_index()\n",
    "user_total_products.columns = ['user_id', 'user_total_products']\n",
    "\n",
    "# Average basket size per user\n",
    "user_avg_basket_size = user_products_count.groupby('user_id')['product_id'].mean().reset_index()\n",
    "user_avg_basket_size.columns = ['user_id', 'user_avg_basket_size']\n",
    "\n",
    "# 6. Unique products per user\n",
    "user_unique_products = order_products_prior.merge(prior_orders[['order_id', 'user_id']], on='order_id')\n",
    "user_unique_products_count = user_unique_products.groupby('user_id')['product_id'].nunique().reset_index()\n",
    "user_unique_products_count.columns = ['user_id', 'user_unique_products']\n",
    "\n",
    "# 7. Tá»‰ lá»‡ reorder chung cá»§a user\n",
    "user_reorder_rate = user_unique_products.groupby('user_id')['reordered'].mean().reset_index()\n",
    "user_reorder_rate.columns = ['user_id', 'user_reorder_rate']\n",
    "\n",
    "# Combine all user features\n",
    "user_features = user_orders_count\n",
    "user_features = user_features.merge(user_avg_days_between_orders, on='user_id', how='left')\n",
    "user_features = user_features.merge(user_days_stats, on='user_id', how='left')\n",
    "user_features = user_features.merge(user_hour_mode, on='user_id', how='left')\n",
    "user_features = user_features.merge(user_dow_mode, on='user_id', how='left')\n",
    "user_features = user_features.merge(user_total_products, on='user_id', how='left')\n",
    "user_features = user_features.merge(user_avg_basket_size, on='user_id', how='left')\n",
    "user_features = user_features.merge(user_unique_products_count, on='user_id', how='left')\n",
    "user_features = user_features.merge(user_reorder_rate, on='user_id', how='left')\n",
    "\n",
    "# Fill missing values\n",
    "user_features = user_features.fillna(0)\n",
    "\n",
    "print(f\"âœ“ Created user-level features for {user_features.shape[0]:,} users\")\n",
    "print(f\"  Features shape: {user_features.shape}\")\n",
    "print(f\"  Feature columns: {list(user_features.columns)}\")\n",
    "\n",
    "# Display sample statistics\n",
    "print(f\"\\nUser Features Summary:\")\n",
    "print(user_features.describe())\n",
    "user_features.to_csv(data_processed / \"user_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09c4d1c",
   "metadata": {},
   "source": [
    "## 3. XÃ¢y dá»±ng Item-Level Features\n",
    "\n",
    "**Item-level features** mÃ´ táº£ Ä‘áº·c Ä‘iá»ƒm cá»§a tá»«ng sáº£n pháº©m:\n",
    "- Äá»™ phá»• biáº¿n cá»§a sáº£n pháº©m\n",
    "- Tá»‰ lá»‡ reorder cá»§a sáº£n pháº©m\n",
    "- Má»©c Ä‘á»™ Ä‘Æ°á»£c thÃªm vÃ o giá» (add_to_cart_order)\n",
    "- Thá»i Ä‘iá»ƒm sáº£n pháº©m thÆ°á»ng Ä‘Æ°á»£c mua"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d224237e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BUILDING ITEM-LEVEL FEATURES\n",
      "========================================\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 36\u001b[39m\n\u001b[32m     33\u001b[39m product_preferred_dow.columns = [\u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mproduct_preferred_dow\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 5. Sá»‘ lÆ°á»£ng users Ä‘Ã£ mua sáº£n pháº©m nÃ y\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m product_users_count = \u001b[43morder_products_with_time\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mproduct_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43morder_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprior_orders\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprior_orders\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43morder_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43muser_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m)\u001b[49m.reset_index()\n\u001b[32m     39\u001b[39m product_users_count.columns = [\u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mproduct_unique_users\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# 6. Department vÃ  Aisle features\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\groupby\\generic.py:230\u001b[39m, in \u001b[36mSeriesGroupBy.apply\u001b[39m\u001b[34m(self, func, *args, **kwargs)\u001b[39m\n\u001b[32m    224\u001b[39m \u001b[38;5;129m@Appender\u001b[39m(\n\u001b[32m    225\u001b[39m     _apply_docs[\u001b[33m\"\u001b[39m\u001b[33mtemplate\u001b[39m\u001b[33m\"\u001b[39m].format(\n\u001b[32m    226\u001b[39m         \u001b[38;5;28minput\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mseries\u001b[39m\u001b[33m\"\u001b[39m, examples=_apply_docs[\u001b[33m\"\u001b[39m\u001b[33mseries_examples\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    227\u001b[39m     )\n\u001b[32m    228\u001b[39m )\n\u001b[32m    229\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, *args, **kwargs) -> Series:\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1826\u001b[39m, in \u001b[36mGroupBy.apply\u001b[39m\u001b[34m(self, func, include_groups, *args, **kwargs)\u001b[39m\n\u001b[32m   1824\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[33m\"\u001b[39m\u001b[33mmode.chained_assignment\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1825\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1826\u001b[39m         result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_python_apply_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_selected_obj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1827\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1828\u001b[39m             \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.obj, Series)\n\u001b[32m   1829\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1830\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._selected_obj.shape != \u001b[38;5;28mself\u001b[39m._obj_with_exclusions.shape\n\u001b[32m   1831\u001b[39m         ):\n\u001b[32m   1832\u001b[39m             warnings.warn(\n\u001b[32m   1833\u001b[39m                 message=_apply_groupings_depr.format(\n\u001b[32m   1834\u001b[39m                     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).\u001b[34m__name__\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mapply\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1837\u001b[39m                 stacklevel=find_stack_level(),\n\u001b[32m   1838\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\groupby\\groupby.py:1887\u001b[39m, in \u001b[36mGroupBy._python_apply_general\u001b[39m\u001b[34m(self, f, data, not_indexed_same, is_transform, is_agg)\u001b[39m\n\u001b[32m   1852\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   1853\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_python_apply_general\u001b[39m(\n\u001b[32m   1854\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1859\u001b[39m     is_agg: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   1860\u001b[39m ) -> NDFrameT:\n\u001b[32m   1861\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1862\u001b[39m \u001b[33;03m    Apply function f in python space\u001b[39;00m\n\u001b[32m   1863\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1885\u001b[39m \u001b[33;03m        data after applying f\u001b[39;00m\n\u001b[32m   1886\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1887\u001b[39m     values, mutated = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_grouper\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_groupwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1888\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m not_indexed_same \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1889\u001b[39m         not_indexed_same = mutated\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\groupby\\ops.py:928\u001b[39m, in \u001b[36mBaseGrouper.apply_groupwise\u001b[39m\u001b[34m(self, f, data, axis)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;66;03m# group might be modified\u001b[39;00m\n\u001b[32m    927\u001b[39m group_axes = group.axes\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m res = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mutated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_indexed_like(res, group_axes, axis):\n\u001b[32m    930\u001b[39m     mutated = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36m<lambda>\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m     33\u001b[39m product_preferred_dow.columns = [\u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mproduct_preferred_dow\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# 5. Sá»‘ lÆ°á»£ng users Ä‘Ã£ mua sáº£n pháº©m nÃ y\u001b[39;00m\n\u001b[32m     36\u001b[39m product_users_count = order_products_with_time.groupby(\u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m)[\u001b[33m'\u001b[39m\u001b[33morder_id\u001b[39m\u001b[33m'\u001b[39m].apply(\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(prior_orders[\u001b[43mprior_orders\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43morder_id\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m][\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m].unique())\n\u001b[32m     38\u001b[39m ).reset_index()\n\u001b[32m     39\u001b[39m product_users_count.columns = [\u001b[33m'\u001b[39m\u001b[33mproduct_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mproduct_unique_users\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# 6. Department vÃ  Aisle features\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\series.py:5578\u001b[39m, in \u001b[36mSeries.isin\u001b[39m\u001b[34m(self, values)\u001b[39m\n\u001b[32m   5505\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34misin\u001b[39m(\u001b[38;5;28mself\u001b[39m, values) -> Series:\n\u001b[32m   5506\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5507\u001b[39m \u001b[33;03m    Whether elements in Series are contained in `values`.\u001b[39;00m\n\u001b[32m   5508\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5576\u001b[39m \u001b[33;03m    dtype: bool\u001b[39;00m\n\u001b[32m   5577\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5578\u001b[39m     result = \u001b[43malgorithms\u001b[49m\u001b[43m.\u001b[49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   5579\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor(result, index=\u001b[38;5;28mself\u001b[39m.index, copy=\u001b[38;5;28;01mFalse\u001b[39;00m).__finalize__(\n\u001b[32m   5580\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33misin\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   5581\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\algorithms.py:545\u001b[39m, in \u001b[36misin\u001b[39m\u001b[34m(comps, values)\u001b[39m\n\u001b[32m    542\u001b[39m     comps_array = comps_array.astype(common, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    543\u001b[39m     f = htable.ismember\n\u001b[32m--> \u001b[39m\u001b[32m545\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomps_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\core\\algorithms.py:537\u001b[39m, in \u001b[36misin.<locals>.<lambda>\u001b[39m\u001b[34m(a, b)\u001b[39m\n\u001b[32m    534\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m np.logical_or(np.isin(c, v).ravel(), np.isnan(c))\n\u001b[32m    536\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m537\u001b[39m         f = \u001b[38;5;28;01mlambda\u001b[39;00m a, b: \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m.ravel()\n\u001b[32m    539\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    540\u001b[39m     common = np_find_common_type(values.dtype, comps_array.dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py:1177\u001b[39m, in \u001b[36misin\u001b[39m\u001b[34m(element, test_elements, assume_unique, invert, kind)\u001b[39m\n\u001b[32m   1063\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1064\u001b[39m \u001b[33;03mCalculates ``element in test_elements``, broadcasting over `element` only.\u001b[39;00m\n\u001b[32m   1065\u001b[39m \u001b[33;03mReturns a boolean array of the same shape as `element` that is True\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1174\u001b[39m \u001b[33;03m       [ True, False]])\u001b[39;00m\n\u001b[32m   1175\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1176\u001b[39m element = np.asarray(element)\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_in1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43melement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massume_unique\u001b[49m\u001b[43m=\u001b[49m\u001b[43massume_unique\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m             \u001b[49m\u001b[43minvert\u001b[49m\u001b[43m=\u001b[49m\u001b[43minvert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m)\u001b[49m.reshape(element.shape)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hoang\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\numpy\\lib\\_arraysetops_impl.py:994\u001b[39m, in \u001b[36m_in1d\u001b[39m\u001b[34m(ar1, ar2, assume_unique, invert, kind)\u001b[39m\n\u001b[32m    990\u001b[39m         dtype = ar2.dtype\n\u001b[32m    992\u001b[39m     out = np.empty_like(in_range_ar1, dtype=np.intp)\n\u001b[32m    993\u001b[39m     outgoing_array[basic_mask] = isin_helper_ar[\n\u001b[32m--> \u001b[39m\u001b[32m994\u001b[39m             \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msubtract\u001b[49m\u001b[43m(\u001b[49m\u001b[43min_range_ar1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mar2_min\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    995\u001b[39m \u001b[43m                        \u001b[49m\u001b[43mout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43munsafe\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m]\n\u001b[32m    997\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m outgoing_array\n\u001b[32m    998\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m kind == \u001b[33m'\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m'\u001b[39m:  \u001b[38;5;66;03m# not range_safe_from_overflow\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"BUILDING ITEM-LEVEL FEATURES\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 1. Äá»™ phá»• biáº¿n cá»§a sáº£n pháº©m (Product popularity)\n",
    "product_popularity = order_products_prior['product_id'].value_counts().reset_index()\n",
    "product_popularity.columns = ['product_id', 'product_total_orders']\n",
    "\n",
    "# 2. Tá»‰ lá»‡ reorder cá»§a sáº£n pháº©m\n",
    "product_reorder_rate = order_products_prior.groupby('product_id')['reordered'].agg(['mean', 'sum', 'count']).reset_index()\n",
    "product_reorder_rate.columns = ['product_id', 'product_reorder_rate', 'product_reorder_count', 'product_total_purchases']\n",
    "\n",
    "# 3. Vá»‹ trÃ­ trung bÃ¬nh trong giá» hÃ ng (add_to_cart_order)\n",
    "product_cart_position = order_products_prior.groupby('product_id')['add_to_cart_order'].mean().reset_index()\n",
    "product_cart_position.columns = ['product_id', 'product_avg_cart_position']\n",
    "\n",
    "# 4. Thá»i Ä‘iá»ƒm sáº£n pháº©m thÆ°á»ng Ä‘Æ°á»£c mua\n",
    "# Merge vá»›i orders Ä‘á»ƒ cÃ³ thÃ´ng tin thá»i gian\n",
    "order_products_with_time = order_products_prior.merge(\n",
    "    prior_orders[['order_id', 'order_hour_of_day', 'order_dow']], \n",
    "    on='order_id'\n",
    ")\n",
    "\n",
    "# Giá» Æ°a thÃ­ch mua sáº£n pháº©m\n",
    "product_preferred_hour = order_products_with_time.groupby('product_id')['order_hour_of_day'].apply(\n",
    "    lambda x: x.mode().iloc[0] if not x.empty else 10\n",
    ").reset_index()\n",
    "product_preferred_hour.columns = ['product_id', 'product_preferred_hour']\n",
    "\n",
    "# NgÃ y trong tuáº§n Æ°a thÃ­ch\n",
    "product_preferred_dow = order_products_with_time.groupby('product_id')['order_dow'].apply(\n",
    "    lambda x: x.mode().iloc[0] if not x.empty else 0\n",
    ").reset_index()\n",
    "product_preferred_dow.columns = ['product_id', 'product_preferred_dow']\n",
    "\n",
    "# 5. Sá»‘ lÆ°á»£ng users Ä‘Ã£ mua sáº£n pháº©m nÃ y\n",
    "product_users_count = order_products_with_time.groupby('product_id')['order_id'].apply(\n",
    "    lambda x: len(prior_orders[prior_orders['order_id'].isin(x)]['user_id'].unique())\n",
    ").reset_index()\n",
    "product_users_count.columns = ['product_id', 'product_unique_users']\n",
    "\n",
    "# 6. Department vÃ  Aisle features\n",
    "products_with_categories = products.merge(departments, on='department_id', how='left')\n",
    "products_with_categories = products_with_categories.merge(aisles, on='aisle_id', how='left')\n",
    "\n",
    "# Combine all product features\n",
    "product_features = products_with_categories[['product_id', 'aisle_id', 'department_id']]\n",
    "product_features = product_features.merge(product_popularity, on='product_id', how='left')\n",
    "product_features = product_features.merge(product_reorder_rate, on='product_id', how='left')\n",
    "product_features = product_features.merge(product_cart_position, on='product_id', how='left')\n",
    "product_features = product_features.merge(product_preferred_hour, on='product_id', how='left')\n",
    "product_features = product_features.merge(product_preferred_dow, on='product_id', how='left')\n",
    "product_features = product_features.merge(product_users_count, on='product_id', how='left')\n",
    "\n",
    "# Fill missing values\n",
    "product_features = product_features.fillna(0)\n",
    "\n",
    "print(f\"âœ“ Created item-level features for {product_features.shape[0]:,} products\")\n",
    "print(f\"  Features shape: {product_features.shape}\")\n",
    "print(f\"  Feature columns: {list(product_features.columns)}\")\n",
    "\n",
    "# Display sample statistics\n",
    "print(f\"\\nProduct Features Summary:\")\n",
    "print(product_features.describe())\n",
    "\n",
    "# Save product features\n",
    "print(f\"\\nğŸ’¾ Saving product features to processed data...\")\n",
    "product_features.to_csv(data_processed / \"product_features.csv\", index=False)\n",
    "product_features.to_parquet(data_processed / \"product_features.parquet\", index=False)\n",
    "print(f\"âœ“ Product features saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05cd561",
   "metadata": {},
   "source": [
    "## 4. XÃ¢y dá»±ng User-Product Interaction Features\n",
    "\n",
    "**User-Product Interaction features** mÃ´ táº£ má»‘i quan há»‡ giá»¯a user vÃ  sáº£n pháº©m:\n",
    "- Lá»‹ch sá»­ mua hÃ ng user-product\n",
    "- Thá»i gian tá»« láº§n mua cuá»‘i\n",
    "- Táº§n suáº¥t mua sáº£n pháº©m cá»¥ thá»ƒ\n",
    "- Vá»‹ trÃ­ trung bÃ¬nh sáº£n pháº©m trong giá» hÃ ng cá»§a user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c4032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BUILDING USER-PRODUCT INTERACTION FEATURES\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Láº¥y dá»¯ liá»‡u train Ä‘á»ƒ táº¡o target\n",
    "train_orders = orders[orders['eval_set'] == 'train'].copy()\n",
    "train_data = order_products_train.merge(train_orders[['order_id', 'user_id']], on='order_id')\n",
    "\n",
    "print(f\"Train data shape: {train_data.shape}\")\n",
    "print(f\"Train users: {train_data['user_id'].nunique():,}\")\n",
    "\n",
    "# Táº¡o user-product interaction features tá»« prior orders\n",
    "user_product_prior = order_products_prior.merge(\n",
    "    prior_orders[['order_id', 'user_id', 'order_number']], \n",
    "    on='order_id'\n",
    ")\n",
    "\n",
    "print(\"Creating user-product interaction features...\")\n",
    "\n",
    "# 1. Lá»‹ch sá»­ mua hÃ ng user-product (Ä‘Ã£ tá»«ng mua hay chÆ°a)\n",
    "user_product_history = user_product_prior.groupby(['user_id', 'product_id']).agg({\n",
    "    'order_id': 'count',  # sá»‘ láº§n mua\n",
    "    'reordered': 'sum',   # sá»‘ láº§n reorder\n",
    "    'add_to_cart_order': 'mean',  # vá»‹ trÃ­ trung bÃ¬nh trong giá»\n",
    "    'order_number': 'max'  # láº§n mua cuá»‘i cÃ¹ng\n",
    "}).reset_index()\n",
    "\n",
    "user_product_history.columns = ['user_id', 'product_id', 'up_orders_count', \n",
    "                                'up_reorder_count', 'up_avg_cart_position', \n",
    "                                'up_last_order_number']\n",
    "\n",
    "# 2. Thá»i gian tá»« láº§n mua cuá»‘i (days since last purchase)\n",
    "user_max_order_number = prior_orders.groupby('user_id')['order_number'].max().reset_index()\n",
    "user_max_order_number.columns = ['user_id', 'user_max_order_number']\n",
    "\n",
    "user_product_history = user_product_history.merge(user_max_order_number, on='user_id')\n",
    "user_product_history['up_days_since_last_purchase'] = user_product_history['user_max_order_number'] - user_product_history['up_last_order_number']\n",
    "\n",
    "# 3. Tá»‰ lá»‡ user mua sáº£n pháº©m nÃ y (order frequency ratio)\n",
    "user_product_history = user_product_history.merge(\n",
    "    user_features[['user_id', 'user_total_orders']], on='user_id'\n",
    ")\n",
    "user_product_history['up_order_rate'] = user_product_history['up_orders_count'] / user_product_history['user_total_orders']\n",
    "\n",
    "# 4. Tá»‰ lá»‡ reorder cá»§a user cho sáº£n pháº©m nÃ y\n",
    "user_product_history['up_reorder_rate'] = user_product_history['up_reorder_count'] / user_product_history['up_orders_count']\n",
    "user_product_history['up_reorder_rate'] = user_product_history['up_reorder_rate'].fillna(0)\n",
    "\n",
    "print(f\"âœ“ Created user-product interaction features\")\n",
    "print(f\"  Shape: {user_product_history.shape}\")\n",
    "print(f\"  Unique user-product pairs: {user_product_history.shape[0]:,}\")\n",
    "\n",
    "# Display sample\n",
    "print(f\"\\nUser-Product Features Sample:\")\n",
    "print(user_product_history.head())\n",
    "\n",
    "# Save user-product features\n",
    "print(f\"\\nğŸ’¾ Saving user-product interaction features...\")\n",
    "user_product_history.to_csv(data_processed / \"user_product_features.csv\", index=False)\n",
    "user_product_history.to_parquet(data_processed / \"user_product_features.parquet\", index=False)\n",
    "print(f\"âœ“ User-product features saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b34b4",
   "metadata": {},
   "source": [
    "## 5. Táº¡o Training Dataset cho Machine Learning\n",
    "\n",
    "Káº¿t há»£p táº¥t cáº£ features Ä‘á»ƒ táº¡o dataset cuá»‘i cÃ¹ng cho training:\n",
    "- Merge user features, product features, vÃ  user-product interaction features\n",
    "- Táº¡o target variable (reordered = 1/0)\n",
    "- Táº¡o negative samples (sáº£n pháº©m user chÆ°a tá»«ng mua)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad7a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CREATING FINAL TRAINING DATASET\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 1. Táº¡o positive samples tá»« train data (nhá»¯ng sáº£n pháº©m Ä‘Æ°á»£c reorder)\n",
    "positive_samples = train_data[['user_id', 'product_id', 'reordered']].copy()\n",
    "print(f\"Positive samples: {positive_samples.shape[0]:,}\")\n",
    "print(f\"Reordered products: {positive_samples['reordered'].sum():,}\")\n",
    "print(f\"Reorder rate in train: {positive_samples['reordered'].mean():.3f}\")\n",
    "\n",
    "# 2. Merge vá»›i user features\n",
    "final_dataset = positive_samples.merge(user_features, on='user_id', how='left')\n",
    "print(f\"After merging user features: {final_dataset.shape}\")\n",
    "\n",
    "# 3. Merge vá»›i product features  \n",
    "final_dataset = final_dataset.merge(product_features, on='product_id', how='left')\n",
    "print(f\"After merging product features: {final_dataset.shape}\")\n",
    "\n",
    "# 4. Merge vá»›i user-product interaction features\n",
    "final_dataset = final_dataset.merge(user_product_history, on=['user_id', 'product_id'], how='left')\n",
    "print(f\"After merging user-product features: {final_dataset.shape}\")\n",
    "\n",
    "# 5. Fill missing values for products that users never bought before\n",
    "interaction_features = ['up_orders_count', 'up_reorder_count', 'up_avg_cart_position', \n",
    "                       'up_last_order_number', 'up_days_since_last_purchase', \n",
    "                       'up_order_rate', 'up_reorder_rate']\n",
    "\n",
    "for feature in interaction_features:\n",
    "    if feature in final_dataset.columns:\n",
    "        if 'rate' in feature or 'position' in feature:\n",
    "            final_dataset[feature] = final_dataset[feature].fillna(0)\n",
    "        elif 'days_since' in feature:\n",
    "            final_dataset[feature] = final_dataset[feature].fillna(999)  # very large number for never bought\n",
    "        else:\n",
    "            final_dataset[feature] = final_dataset[feature].fillna(0)\n",
    "\n",
    "# Fill other missing values\n",
    "final_dataset = final_dataset.fillna(0)\n",
    "\n",
    "print(f\"\\nâœ“ Final training dataset created!\")\n",
    "print(f\"  Shape: {final_dataset.shape}\")\n",
    "print(f\"  Features: {final_dataset.shape[1] - 1}\")  # -1 for target column\n",
    "print(f\"  Users: {final_dataset['user_id'].nunique():,}\")\n",
    "print(f\"  Products: {final_dataset['product_id'].nunique():,}\")\n",
    "print(f\"  Reorder rate: {final_dataset['reordered'].mean():.3f}\")\n",
    "\n",
    "# Display feature list\n",
    "feature_columns = [col for col in final_dataset.columns if col not in ['user_id', 'product_id', 'reordered']]\n",
    "print(f\"\\nFeature columns ({len(feature_columns)}):\")\n",
    "for i, feature in enumerate(feature_columns, 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample data:\")\n",
    "print(final_dataset.head())\n",
    "\n",
    "# Display statistics\n",
    "print(f\"\\nDataset statistics:\")\n",
    "print(final_dataset.describe())\n",
    "\n",
    "# Check for missing values\n",
    "missing_counts = final_dataset.isnull().sum()\n",
    "if missing_counts.sum() > 0:\n",
    "    print(f\"\\nâš ï¸ Missing values found:\")\n",
    "    print(missing_counts[missing_counts > 0])\n",
    "else:\n",
    "    print(f\"\\nâœ“ No missing values found!\")\n",
    "\n",
    "print(f\"\\nğŸ’¾ Saving final training dataset...\")\n",
    "final_dataset.to_csv(data_processed / \"final_training_dataset.csv\", index=False)\n",
    "final_dataset.to_parquet(data_processed / \"final_training_dataset.parquet\", index=False)\n",
    "print(f\"âœ“ Final dataset saved successfully!\")\n",
    "\n",
    "# Memory usage\n",
    "memory_usage = final_dataset.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"ğŸ“Š Final dataset memory usage: {memory_usage:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0039c9a5",
   "metadata": {},
   "source": [
    "## 6. Feature Importance Analysis & Visualization\n",
    "\n",
    "PhÃ¢n tÃ­ch táº§m quan trá»ng cá»§a cÃ¡c features vÃ  visualize relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4e2be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "# Prepare data for analysis\n",
    "analysis_data = final_dataset.copy()\n",
    "feature_cols = [col for col in analysis_data.columns if col not in ['user_id', 'product_id', 'reordered']]\n",
    "X = analysis_data[feature_cols]\n",
    "y = analysis_data['reordered']\n",
    "\n",
    "print(f\"Features for analysis: {len(feature_cols)}\")\n",
    "print(f\"Samples: {len(X):,}\")\n",
    "\n",
    "# 1. Random Forest Feature Importance\n",
    "print(\"\\nğŸŒ² Computing Random Forest feature importance...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# Use a sample for faster computation\n",
    "sample_size = min(50000, len(X))\n",
    "sample_idx = np.random.choice(len(X), sample_size, replace=False)\n",
    "X_sample = X.iloc[sample_idx]\n",
    "y_sample = y.iloc[sample_idx]\n",
    "\n",
    "rf.fit(X_sample, y_sample)\n",
    "rf_importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 15 Random Forest Features:\")\n",
    "print(rf_importance.head(15))\n",
    "\n",
    "# 2. Correlation with target\n",
    "print(\"\\nğŸ“Š Computing feature correlations with target...\")\n",
    "correlations = []\n",
    "for col in feature_cols:\n",
    "    corr = np.corrcoef(X[col], y)[0, 1]\n",
    "    correlations.append({'feature': col, 'correlation': abs(corr)})\n",
    "\n",
    "correlation_df = pd.DataFrame(correlations).sort_values('correlation', ascending=False)\n",
    "print(\"Top 15 Correlated Features:\")\n",
    "print(correlation_df.head(15))\n",
    "\n",
    "# 3. Feature distributions by target\n",
    "print(\"\\nğŸ“ˆ Creating feature importance visualization...\")\n",
    "\n",
    "# Plot top features\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# Random Forest Importance\n",
    "top_rf_features = rf_importance.head(15)\n",
    "axes[0, 0].barh(range(len(top_rf_features)), top_rf_features['importance'])\n",
    "axes[0, 0].set_yticks(range(len(top_rf_features)))\n",
    "axes[0, 0].set_yticklabels(top_rf_features['feature'])\n",
    "axes[0, 0].set_title('Top 15 Random Forest Feature Importance')\n",
    "axes[0, 0].set_xlabel('Importance')\n",
    "\n",
    "# Correlation with target\n",
    "top_corr_features = correlation_df.head(15)\n",
    "axes[0, 1].barh(range(len(top_corr_features)), top_corr_features['correlation'])\n",
    "axes[0, 1].set_yticks(range(len(top_corr_features)))\n",
    "axes[0, 1].set_yticklabels(top_corr_features['feature'])\n",
    "axes[0, 1].set_title('Top 15 Feature Correlations with Target')\n",
    "axes[0, 1].set_xlabel('Absolute Correlation')\n",
    "\n",
    "# Distribution of top feature by target\n",
    "top_feature = rf_importance.iloc[0]['feature']\n",
    "reorder_yes = analysis_data[analysis_data['reordered'] == 1][top_feature]\n",
    "reorder_no = analysis_data[analysis_data['reordered'] == 0][top_feature]\n",
    "\n",
    "axes[1, 0].hist(reorder_no, bins=50, alpha=0.7, label='Not Reordered', density=True)\n",
    "axes[1, 0].hist(reorder_yes, bins=50, alpha=0.7, label='Reordered', density=True)\n",
    "axes[1, 0].set_title(f'Distribution of {top_feature}')\n",
    "axes[1, 0].set_xlabel(top_feature)\n",
    "axes[1, 0].set_ylabel('Density')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "# Feature correlation heatmap (top features only)\n",
    "top_features_for_corr = rf_importance.head(10)['feature'].tolist()\n",
    "corr_matrix = analysis_data[top_features_for_corr].corr()\n",
    "im = axes[1, 1].imshow(corr_matrix, cmap='coolwarm', aspect='auto', vmin=-1, vmax=1)\n",
    "axes[1, 1].set_xticks(range(len(top_features_for_corr)))\n",
    "axes[1, 1].set_yticks(range(len(top_features_for_corr)))\n",
    "axes[1, 1].set_xticklabels(top_features_for_corr, rotation=45, ha='right')\n",
    "axes[1, 1].set_yticklabels(top_features_for_corr)\n",
    "axes[1, 1].set_title('Feature Correlation Matrix (Top 10)')\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. Save feature importance results\n",
    "print(\"\\nğŸ’¾ Saving feature importance analysis...\")\n",
    "rf_importance.to_csv(data_processed / \"feature_importance_rf.csv\", index=False)\n",
    "correlation_df.to_csv(data_processed / \"feature_correlation.csv\", index=False)\n",
    "\n",
    "# Feature summary\n",
    "feature_summary = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'rf_importance': rf_importance.set_index('feature').loc[feature_cols, 'importance'].values,\n",
    "    'correlation': correlation_df.set_index('feature').loc[feature_cols, 'correlation'].values,\n",
    "})\n",
    "feature_summary['combined_score'] = (feature_summary['rf_importance'] * 0.6 + \n",
    "                                   feature_summary['correlation'] * 0.4)\n",
    "feature_summary = feature_summary.sort_values('combined_score', ascending=False)\n",
    "\n",
    "print(\"\\nTop 20 Features (Combined Score):\")\n",
    "print(feature_summary.head(20))\n",
    "\n",
    "feature_summary.to_csv(data_processed / \"feature_summary.csv\", index=False)\n",
    "print(\"âœ“ Feature analysis saved successfully!\")\n",
    "\n",
    "print(f\"\\nğŸ¯ FEATURE ENGINEERING COMPLETE!\")\n",
    "print(f\"ğŸ“ All processed data saved to: {data_processed}\")\n",
    "print(f\"ğŸ“Š Ready for model training!\")\n",
    "\n",
    "# Summary statistics\n",
    "print(f\"\\nğŸ“ˆ FINAL SUMMARY:\")\n",
    "print(f\"  ğŸ‘¥ Users: {final_dataset['user_id'].nunique():,}\")\n",
    "print(f\"  ğŸ›’ Products: {final_dataset['product_id'].nunique():,}\")\n",
    "print(f\"  ğŸ“ Training samples: {final_dataset.shape[0]:,}\")\n",
    "print(f\"  ğŸ”¢ Features: {len(feature_cols)}\")\n",
    "print(f\"  ğŸ¯ Reorder rate: {final_dataset['reordered'].mean():.1%}\")\n",
    "print(f\"  ğŸ’¾ Dataset size: {final_dataset.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
